{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c806b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import ipywidgets\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc9149",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f7f3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 223 # it's my birthday\n",
    "    gpu = 0 \n",
    "    \n",
    "    val_ratio = 0.3\n",
    "    batch_size = 24\n",
    "    epochs = 50\n",
    "    \n",
    "    lr = 0.1\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be31940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f'cuda:{config.gpu}' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62696bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_seed(random_seed: int = 223):\n",
    "    \"\"\"\n",
    "    set deterministic seed\n",
    "\n",
    "    Argument\n",
    "    --------\n",
    "    - random_seed : random seed number, default 223 is my birth day. haha\n",
    "\n",
    "    \"\"\"\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11cfd05",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff64e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = pd.read_csv('../data/FIW/train-pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bfffbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c297b43e80c142b58742e66653a6d2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6983.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img_path = []\n",
    "\n",
    "for i in tqdm(range(len(train_info))):\n",
    "    # image path\n",
    "    p1_path = glob(f'../data/FIW/train-faces/{train_info.p1.tolist()[i]}/*.jpg')\n",
    "    p2_path = glob(f'../data/FIW/train-faces/{train_info.p2.tolist()[i]}/*.jpg')\n",
    "    img_path.extend(list(product(p1_path, p2_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24ba3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = pd.DataFrame(img_path, columns=['p1','p2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef0f73",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629b34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datadir = '../data/FIW/train-faces'\n",
    "\n",
    "family_list = os.listdir(train_datadir)\n",
    "family_dropdown = ipywidgets.Dropdown(options=family_list)\n",
    "\n",
    "member_list = os.listdir(os.path.join(train_datadir, family_dropdown.value))\n",
    "member_list = [m for m in member_list if 'MID' in m]\n",
    "member_dropdown = ipywidgets.Dropdown(options=member_list)\n",
    "\n",
    "member_i_list = os.listdir(os.path.join(train_datadir, family_dropdown.value, member_dropdown.value))\n",
    "member_i_dropdown = ipywidgets.Dropdown(options=member_i_list)\n",
    "\n",
    "def family_change(change):\n",
    "    member_list = os.listdir(os.path.join(train_datadir, change.new))\n",
    "    member_list = [m for m in member_list if 'MID' in m]\n",
    "    member_dropdown.options = member_list\n",
    "    \n",
    "def member_change(change):\n",
    "    member_i_dropdown.options = os.listdir(os.path.join(train_datadir, family_dropdown.value, change.new))\n",
    "                                     \n",
    "family_dropdown.observe(family_change, names='value')\n",
    "member_dropdown.observe(member_change, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03b4e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(family, member, member_i):\n",
    "    img = Image.open(f'../data/FIW/train-faces/{family}/{member}/{member_i}')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c860132",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9f44764d5049269f7739c488e8480e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='family', options=('F0774', 'F0288', 'F0612', 'F0389', 'F0942', 'F0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipywidgets.interactive(plot_img, \n",
    "                       family=family_dropdown,\n",
    "                       member=member_dropdown,\n",
    "                       member_i=member_i_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239104b2",
   "metadata": {},
   "source": [
    "## Family Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20a809b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of family: 571 -> 50\n",
      "Family size: 264540 -> 10164\n"
     ]
    }
   ],
   "source": [
    "# random sampling\n",
    "torch_seed(config.seed)\n",
    "\n",
    "img_path['FID'] = img_path.p1.apply(lambda x: x.split('/')[-3])\n",
    "sample_fid = np.random.choice(img_path['FID'].unique(), size=50, replace=False)\n",
    "\n",
    "sampled_img_path = img_path[img_path['FID'].isin(sample_fid)]\n",
    "raw_len = img_path.shape[0]\n",
    "sample_len = sampled_img_path.shape[0]\n",
    "\n",
    "print(f'The number of family: {img_path.FID.nunique()} -> {sampled_img_path.FID.nunique()}')\n",
    "print(f'Family size: {raw_len} -> {sample_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a0e915",
   "metadata": {},
   "source": [
    "## Split Train and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c67a6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:  (7115, 3)\n",
      "Validation Size:  (3049, 3)\n"
     ]
    }
   ],
   "source": [
    "# random shuffle\n",
    "torch_seed(config.seed)\n",
    "val_path = sampled_img_path.sample(frac=config.val_ratio)\n",
    "train_path = sampled_img_path.drop(val_path.index, axis=0)\n",
    "\n",
    "print('Train Size: ',train_path.shape)\n",
    "print('Validation Size: ',val_path.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ebece6",
   "metadata": {},
   "source": [
    "## Build Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f396555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinshipDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_path, transform):\n",
    "        self.p1 = img_path['p1'].tolist()\n",
    "        self.p2 = img_path['p2'].tolist()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.p1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # anchor, positive, and negative image path\n",
    "        anchor_path = self.p1[idx]\n",
    "        anchor_fam = anchor_path.split('/')[-2]\n",
    "        \n",
    "        positive_path = self.p2[idx]\n",
    "    \n",
    "        negative_fam = anchor_fam\n",
    "        while anchor_fam == negative_fam:\n",
    "            negative_path = np.random.choice(self.p2, size=1)[0]\n",
    "            negative_fam = negative_path.split('/')[-2]\n",
    "            \n",
    "        # load images and convert RGB to grayscale\n",
    "        anchor = Image.open(anchor_path).convert(\"L\")\n",
    "        positive = Image.open(positive_path).convert(\"L\")\n",
    "        negative = Image.open(negative_path).convert(\"L\")\n",
    "            \n",
    "        # transform\n",
    "        anchor = self.transform(anchor)\n",
    "        positive = self.transform(positive)\n",
    "        negative = self.transform(negative)\n",
    "        \n",
    "        return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6740b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((124,124)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66403ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = KinshipDataset(img_path=train_path, transform=transform)\n",
    "valset = KinshipDataset(img_path=val_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0d10503",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=config.batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=8)\n",
    "valloader = torch.utils.data.DataLoader(valset,\n",
    "                                        batch_size=config.batch_size,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c5a61",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "- ResNet code source : https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
    "- Paper : Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep Residual Learning for Image Recognition. arXiv:1512.03385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d89f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False) # grayscale\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.fcn = torch.nn.Linear(256, 8)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = out.mean(axis=1) # GAP\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fcn(out)\n",
    "        \n",
    "        # L2 normalization\n",
    "        out = torch.nn.functional.normalize(out, p=2, dim=1)\n",
    "        return out\n",
    "    \n",
    "def ResNet18Kinship():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d528ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18Kinship().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701121a5",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7472755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinshipLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(KinshipLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, anchors, positives, negatives):\n",
    "        similarity_loss = -torch.log(torch.sigmoid(torch.einsum('ae,pe -> ap', anchors, positives).diagonal()))\n",
    "        unsimilarity_loss = -torch.log(1 - torch.sigmoid(torch.einsum('ae,pe -> ap', anchors, negatives).diagonal()))\n",
    "        \n",
    "        if self.reduction=='mean':\n",
    "            return (similarity_loss + unsimilarity_loss).mean()\n",
    "        elif self.reduction=='none':\n",
    "            return similarity_loss + unsimilarity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a34f3f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-01.\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "criterion = KinshipLoss()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14f19ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, optimizer, criterion, device):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (anchors, positives, negatives) in enumerate(trainloader):\n",
    "        # inputs and targets\n",
    "        anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n",
    "        \n",
    "        # model output\n",
    "        anchors_output = model(anchors.to(device))\n",
    "        positives_output = model(positives.to(device))\n",
    "        negatives_output = model(negatives.to(device))\n",
    "        \n",
    "        # loss and update\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(anchors_output, positives_output, negatives_output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # history\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx == len(trainloader)-1:\n",
    "            print()\n",
    "        else:\n",
    "            sys.stdout.write(f'\\rStep [{batch_idx+1}/{len(trainloader)}] Loss: {total_loss/(batch_idx+1):.4f}')\n",
    "        \n",
    "    return total_loss / len(trainloader)\n",
    "\n",
    "def test(model, testloader, criterion, device):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (anchors, positives, negatives) in enumerate(testloader):\n",
    "            # inputs and targets\n",
    "            anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n",
    "            \n",
    "\n",
    "            # model output\n",
    "            anchors_output = model(anchors.to(device))\n",
    "            positives_output = model(positives.to(device))\n",
    "            negatives_output = model(negatives.to(device))\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(anchors_output, positives_output, negatives_output)\n",
    "\n",
    "            # history\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch_idx == len(testloader)-1:\n",
    "                print()\n",
    "            else:\n",
    "                sys.stdout.write(f'\\rStep [{batch_idx+1}/{len(testloader)}] Loss: {total_loss/(batch_idx+1):.4f}')\n",
    "        \n",
    "    return total_loss / len(testloader)\n",
    "\n",
    "class CheckPoint:\n",
    "    def __init__(self, savedir):\n",
    "        self.best_loss = 9999\n",
    "        if not os.path.isdir(savedir):\n",
    "            os.makedirs(savedir)\n",
    "    \n",
    "    def step(self, model, loss, epoch):        \n",
    "        if loss < self.best_loss:    \n",
    "            # print\n",
    "            print(f'Model Save : Best Loss {self.best_loss:.4f} -> {loss:.4f}')\n",
    "            \n",
    "            state = {\n",
    "                'net': model.state_dict(),\n",
    "                'loss': loss,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "\n",
    "            torch.save(state, f'../checkpoint/KinshipNet.pth')\n",
    "        \n",
    "            self.best_loss = loss            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded3ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs [1/50]\n",
      "Step [296/297] Loss: 1.4128\n",
      "Step [127/128] Loss: 1.4151\n",
      "Adjusting learning rate of group 0 to 9.9901e-02.\n",
      "Model Save : Best Loss 9999.0000 -> 1.4120\n",
      "\n",
      "Epochs [2/50]\n",
      "Step [296/297] Loss: 1.3949\n",
      "Step [127/128] Loss: 1.3730\n",
      "Adjusting learning rate of group 0 to 9.9606e-02.\n",
      "Model Save : Best Loss 1.4120 -> 1.3758\n",
      "\n",
      "Epochs [3/50]\n",
      "Step [296/297] Loss: 1.3502\n",
      "Step [127/128] Loss: 1.3502\n",
      "Adjusting learning rate of group 0 to 9.9114e-02.\n",
      "Model Save : Best Loss 1.3758 -> 1.3529\n",
      "\n",
      "Epochs [4/50]\n",
      "Step [296/297] Loss: 1.3209\n",
      "Step [127/128] Loss: 1.3169\n",
      "Adjusting learning rate of group 0 to 9.8429e-02.\n",
      "Model Save : Best Loss 1.3529 -> 1.3189\n",
      "\n",
      "Epochs [5/50]\n",
      "Step [296/297] Loss: 1.2889\n",
      "Step [127/128] Loss: 1.2803\n",
      "Adjusting learning rate of group 0 to 9.7553e-02.\n",
      "Model Save : Best Loss 1.3189 -> 1.2815\n",
      "\n",
      "Epochs [6/50]\n",
      "Step [296/297] Loss: 1.2506\n",
      "Step [127/128] Loss: 1.2684\n",
      "Adjusting learning rate of group 0 to 9.6489e-02.\n",
      "Model Save : Best Loss 1.2815 -> 1.2695\n",
      "\n",
      "Epochs [7/50]\n",
      "Step [296/297] Loss: 1.2289\n",
      "Step [127/128] Loss: 1.2302\n",
      "Adjusting learning rate of group 0 to 9.5241e-02.\n",
      "Model Save : Best Loss 1.2695 -> 1.2302\n",
      "\n",
      "Epochs [8/50]\n",
      "Step [296/297] Loss: 1.2047\n",
      "Step [127/128] Loss: 1.2158\n",
      "Adjusting learning rate of group 0 to 9.3815e-02.\n",
      "Model Save : Best Loss 1.2302 -> 1.2143\n",
      "\n",
      "Epochs [9/50]\n",
      "Step [296/297] Loss: 1.1904\n",
      "Step [127/128] Loss: 1.2207\n",
      "Adjusting learning rate of group 0 to 9.2216e-02.\n",
      "\n",
      "Epochs [10/50]\n",
      "Step [296/297] Loss: 1.1810\n",
      "Step [127/128] Loss: 1.1938\n",
      "Adjusting learning rate of group 0 to 9.0451e-02.\n",
      "Model Save : Best Loss 1.2143 -> 1.1925\n",
      "\n",
      "Epochs [11/50]\n",
      "Step [296/297] Loss: 1.1715\n",
      "Step [127/128] Loss: 1.1852\n",
      "Adjusting learning rate of group 0 to 8.8526e-02.\n",
      "Model Save : Best Loss 1.1925 -> 1.1838\n",
      "\n",
      "Epochs [12/50]\n",
      "Step [72/297] Loss: 1.1647"
     ]
    }
   ],
   "source": [
    "torch_seed(config.seed)\n",
    "\n",
    "checkpoint = CheckPoint(savedir='../checkpoint')\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "for epoch in range(config.epochs):\n",
    "    print(f'\\nEpochs [{epoch+1}/{config.epochs}]')\n",
    "    train_loss = train(model, trainloader, optimizer, criterion, device)\n",
    "    val_loss = test(model, valloader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # history\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    \n",
    "    # checkpoint\n",
    "    checkpoint.step(model, val_loss, epoch)\n",
    "    \n",
    "# save history\n",
    "history = {}\n",
    "history['train_loss'] = train_loss_list\n",
    "history['val_loss'] = val_loss_list\n",
    "json.dump(history, open('../checkpoint/history.json','w'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = json.load(open('../checkpoint/history.json','r'))\n",
    "plt.plot(range(len(history['train_loss'])), history['train_loss'])\n",
    "plt.plot(range(len(history['val_loss'])), history['val_loss'])\n",
    "plt.legend(['Train','Validation'], loc='upper right')\n",
    "plt.ylabel('Loss', size=15)\n",
    "plt.xlabel('Epochs', size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7188e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load('../checkpoint/KinshipNet_epoch1.pth')\n",
    "\n",
    "model = ResNet18Kinship().to(device)\n",
    "model.state_dict(weights['net'])\n",
    "model.eval()\n",
    "\n",
    "print('Load pretrained model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03869be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinshipInferDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_path, transform):\n",
    "        self.p = img_path\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.p)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # load image and convert RGB to grayscale\n",
    "        p = Image.open(self.p[idx]).convert(\"L\")\n",
    "            \n",
    "        # transform\n",
    "        p = self.transform(p)\n",
    "        \n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eb0388",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs_path = list(set(val_path['p1'].tolist() + val_path['p2'].tolist()))\n",
    "val_imgs_path.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ac031",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferset = KinshipInferDataset(img_path=val_imgs_path, transform=transform)\n",
    "inferloader = torch.utils.data.DataLoader(inferset,\n",
    "                                          batch_size=256,\n",
    "                                          num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, testloader, device):\n",
    "    p_embed_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, p in enumerate(tqdm(testloader, desc='Inference')):\n",
    "            # inputs and targets\n",
    "            p = p.to(device)\n",
    "            \n",
    "            # model output\n",
    "            output = model(p.to(device))\n",
    "            \n",
    "            # distance\n",
    "            p_embed_list.append(output.cpu().numpy())\n",
    "            \n",
    "    p_embed = np.vstack(p_embed_list)\n",
    "    return p_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ffc0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_embed = inference(model, inferloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a029a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.einsum('ae,pe -> ap', p_embed, p_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_list = [path.split('/')[-3] for path in val_imgs_path]\n",
    "fam_list = pd.Series(fam_list).value_counts().sort_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
